{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open and Partitions the data to smaller chunks\n",
    "\n",
    "The cell below will open the data and partition it into smaller chunks. As it is a large dataset, it is better to work with smaller chunks to avoid memory issues. To make the partitions, the program automatically detects the best way to split the data into smaller bits. It will save the partitions in the `partitions` folder. For each key, it will create a folder in the structure `partitions/key/`. Each folder will contain a parquet file for each unique Month-Year combination in the data.\n",
    "\n",
    "Be aware that this process may take a while to run. The data from keys avgad and avgas are the same, jusf for diffent years.It may be a good idea to copy one to the other, but it is optional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import psutil\n",
    "import os\n",
    "import dask.dataframe as dd\n",
    "import cupy\n",
    "import geopandas as gpd\n",
    "from dask import delayed\n",
    "from dask.diagnostics import ProgressBar\n",
    "from sklearnex import patch_sklearn\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import joblib\n",
    "import optuna\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "import seaborn as sns\n",
    "import scienceplots\n",
    "plt.style.use('science', 'grid', 'notebook')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "era_5_path = \"your_path\"\n",
    "aquasat_path = \"your_path\"\n",
    "keys = [\"avgid\", \"avgas\", \"avgad\", \"avgd\"]\n",
    "output_path = \"your_path\"\n",
    "\n",
    "def get_dataset_size(path):\n",
    "    return os.path.getsize(path)\n",
    "\n",
    "def get_available_memory():\n",
    "    return psutil.virtual_memory().available\n",
    "\n",
    "def calculate_optimal_partitions(dataset_size, available_memory, safety_factor=0.5):\n",
    "    return max(1, int((dataset_size / available_memory) * safety_factor))\n",
    "\n",
    "def open_datasets(era_5_path, aquasat_path, key):\n",
    "    \"\"\"\n",
    "    Opens and filters datasets based on relevant dates.\n",
    "\n",
    "    Parameters:\n",
    "    era_5_path (str): The file path to the ERA-5 dataset.\n",
    "    aquasat_path (str): The file path to the Aquasat dataset.\n",
    "    key (str): The key to filter the ERA-5 dataset by.\n",
    "\n",
    "    Returns:\n",
    "    era_5_filtered (pandas.DataFrame): The filtered ERA-5 dataset.\n",
    "    aquasat (dask.dataframe.DataFrame): The Aquasat dataset.\n",
    "    relevant_dates (pandas.PeriodIndex): The unique relevant dates from the Aquasat dataset.\n",
    "    \"\"\"\n",
    "    if os.path.exists(era_5_path) | os.path.exists(aquasat_path):\n",
    "        raise FileNotFoundError(\"File not found\")\n",
    "    era_5_size = get_dataset_size(era_5_path)\n",
    "    available_memory = get_available_memory()\n",
    "    npartitions = calculate_optimal_partitions(era_5_size, available_memory)\n",
    "    era_5 = xr.open_dataset(era_5_path, engine=\"cfgrib\", chunks={}, filter_by_keys={'stepType': key})\n",
    "    aquasat = dd.read_csv(aquasat_path)\n",
    "    aquasat[\"time\"] = dd.to_datetime(aquasat[\"time\"]).dt.to_period(\"M\")\n",
    "    era_5 = era_5.to_dask_dataframe()\n",
    "    era_5[\"time\"] = era_5[\"time\"].dt.to_period(\"M\")\n",
    "    relevant_dates = aquasat[\"time\"].unique().compute()\n",
    "    era_5_filtered = era_5.loc[era_5[\"time\"].isin(relevant_dates)]\n",
    "    era_5_filtered = era_5_filtered.repartition(npartitions=npartitions)\n",
    "    return era_5_filtered, aquasat, relevant_dates\n",
    "\n",
    "def process_and_save_partition(partition, output, key, relevant_dates):\n",
    "    \"\"\"\n",
    "    Process and save a partition of data to Parquet files.\n",
    "\n",
    "    Args:\n",
    "        partition (pandas.DataFrame): The partition of data to process and save.\n",
    "        output (str): The output directory where the Parquet files will be saved.\n",
    "        key (str): The key used to identify the partition.\n",
    "        relevant_dates (list): A list of relevant dates to filter the data.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    if not partition.empty:\n",
    "        for date in relevant_dates:\n",
    "            era_5_date = partition.loc[partition[\"time\"] == date]\n",
    "            if not era_5_date.empty:\n",
    "                date_str = str(date).replace(\"/\", \"-\")\n",
    "                output_file = f\"{output}{key}_era5/era_5_{date_str}.parquet\"\n",
    "                os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "                era_5_date.to_parquet(output_file, index=True)\n",
    "\n",
    "for key in keys:\n",
    "    era_5, aquasat, relevant_dates = open_datasets(era_5_path, aquasat_path, key)\n",
    "    # Apply the function to each partition\n",
    "    era_5.map_partitions(process_and_save_partition, output=output_path, key=key,\n",
    "                         relevant_dates=relevant_dates, meta=object).compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parquet treatment\n",
    "This step will take the parquet files and pair 1:1 with the aquaset file. The ERA5 dataset contains way more entries thant the aquaset, so this may take a while. The output will be a csv, as it will be way smaller in size. The output will be saved in the format `key_era5_cvs/merged_{date}.csv`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "era_5_path = \"your_path_to_era5_parquet_files\"\n",
    "aquasat_path = \"your_path_to_aquasat.csv\"\n",
    "output_path = \"your_output_path\"\n",
    "key = \"your_key\"\n",
    "# Read the water data as a Dask DataFrame\n",
    "aquasat = dd.read_csv(aquasat_path, assume_missing = True)\n",
    "water[\"time\"] = dd.to_datetime(water[\"time\"]).dt.to_period(\"M\")\n",
    "relevant_dates = water[\"time\"].unique().compute()\n",
    "\n",
    "# Function to process each chunk\n",
    "@delayed\n",
    "def process_chunk(chunk, date_str, date, era_5_path, output_path, key = key):\n",
    "    \"\"\"\n",
    "    Process a chunk of water data, perform a spatial join with ERA-5 data, and save the result to a CSV file.\n",
    "\n",
    "    Parameters:\n",
    "    chunk (pd.DataFrame): The chunk of water data to process.\n",
    "    date_str (str): The date string used for file naming.\n",
    "    date (pd.Period): The date to filter the data by.\n",
    "    era_5_path (str): The path to the ERA-5 data files.\n",
    "    output_path (str): The path to save the output CSV files.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Convert to GeoDataFrame\n",
    "        df_water = gpd.GeoDataFrame(chunk, geometry=gpd.points_from_xy(chunk.longitude, chunk.latitude))\n",
    "        df_water = df_water[df_water[\"time\"] == date]\n",
    "\n",
    "        # Read ERA-5 data\n",
    "        era_5_file = f\"{era_5_path}/era_5_{date_str}.parquet\"\n",
    "        if not os.path.exists(era_5_file):\n",
    "            raise FileNotFoundError(f\"ERA-5 file not found: {era_5_file}\")\n",
    "        \n",
    "        df_era = dd.read_parquet(era_5_file).compute()\n",
    "        df_era = gpd.GeoDataFrame(df_era, geometry=gpd.points_from_xy(df_era.longitude, df_era.latitude))\n",
    "\n",
    "        # Perform spatial join\n",
    "        df_merged = gpd.sjoin_nearest(df_water, df_era, how=\"left\", distance_col=\"distance\")\n",
    "\n",
    "        # Save to CSV\n",
    "        output_file = f\"{output_path}key_era5_csv/merged_{date_str}.csv\"\n",
    "        os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "        df_merged.to_csv(output_file, index=False)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing chunk for date {date_str}: {e}\")\n",
    "\n",
    "# Process each date\n",
    "tasks = []\n",
    "for date in relevant_dates:\n",
    "    date_str = str(date).replace(\"/\", \"-\")\n",
    "    # Process each partition of the Dask DataFrame\n",
    "    for partition in water.to_delayed():\n",
    "        task = process_chunk(partition, date_str, date)\n",
    "        tasks.append(task)\n",
    "\n",
    "# Execute the tasks with progress bar\n",
    "with ProgressBar():\n",
    "    dd.compute(*tasks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminary Clean\n",
    "This step makes a preliminary clean, of the date. It removes some uneeded collumns. The output will be saved in the format `key_era5_cvs/filtered_{date}.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_water_data():\n",
    "       \"\"\"\n",
    "       Reads water data from a CSV file and performs some preprocessing.\n",
    "\n",
    "       Returns:\n",
    "              water (dask.dataframe.DataFrame): A Dask DataFrame containing the water data.\n",
    "       \"\"\"\n",
    "       water = dd.read_csv(\"aquasat_dat.csv\", assume_missing=True)\n",
    "       water[\"time\"] = dd.to_datetime(water[\"time\"]).dt.to_period(\"M\")\n",
    "       return water\n",
    "\n",
    "def filter_merged_data(date, merged_path, merged_cleaned_path, output_name):\n",
    "       \"\"\"\n",
    "       Filter the merged data based on certain conditions and save the filtered data to a CSV file.\n",
    "\n",
    "       Parameters:\n",
    "       - date (str): The date of the merged data file to be filtered.\n",
    "       - merged_path (str): The path to the directory containing the merged data files.\n",
    "       - merged_cleaned_path (str): The path to the directory where the filtered data will be saved.\n",
    "\n",
    "       Returns:\n",
    "       None\n",
    "       \"\"\"\n",
    "       pre_filtered_data = dd.read_csv(f\"{merged_path}/merged_{date}.csv\")\n",
    "       pre_filtered_data = pre_filtered_data[pre_filtered_data[\"doc\"] >= 0]\n",
    "       pre_filtered_data = pre_filtered_data[pre_filtered_data[\"type\"] != \"Facility\"]\n",
    "       post_filter = pre_filtered_data.drop(columns=[\"SiteID\", \"system:index\", \"TZID\",\n",
    "                                                        \"source\", \"landsat_id\",\n",
    "                                                        \"number\", \"step\", \"surface\"])\n",
    "       post_filter.to_csv(f\"{merged_cleaned_path}{output_name}/filtered_{date}.csv\", index=False,\n",
    "                                      single_file=True)\n",
    "\n",
    "def process_water_data():\n",
    "       \"\"\"\n",
    "       Process water data by reading it, extracting relevant dates, and filtering merged data.\n",
    "\n",
    "       Returns:\n",
    "              None\n",
    "       \"\"\"\n",
    "       water = read_water_data()\n",
    "       relevant_dates = water[\"time\"].unique().compute()\n",
    "       merged_path = \"merged_data\"\n",
    "       merged_cleaned_path = \"merged_cleaned\"\n",
    "       output_name = \"key_era5_csv\"\n",
    "       for date in relevant_dates:\n",
    "              filter_merged_data(date, merged_path, merged_cleaned_path,\n",
    "                                 output_name)\n",
    "              \n",
    "\n",
    "process_water_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge all data\n",
    "Merges all the csv files into a single file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def merge_data(input_path, output_path, output_name):\n",
    "    \"\"\"\n",
    "    Merge the filtered data files into a single Parquet file.\n",
    "\n",
    "    Parameters:\n",
    "    - input_path (str): The path to the directory containing the filtered data files.\n",
    "    - output_path (str): The path to the directory where the merged data will be saved.\n",
    "    - output_name (str): The name of the output Parquet file.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    filtered_data = dd.read_csv(f\"{input_path}/*.csv\")\n",
    "    filtered_data.to_csv(f\"{output_path}/{output_name}.csv\", index=False,\n",
    "                         single_file=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removal of band values\n",
    "In this steps we remove all the bands values that are over a absulte distance of a single standart deviation from the mean. This is done to remove outliers. In this step, we also subtract swir2 values from all the other band values and remove locations where the timediff is greater than the threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_medians_and_vars(total_ds, bands):\n",
    "    \"\"\"\n",
    "    Compute the medians and standard deviations for each band in the given dataset.\n",
    "\n",
    "    Parameters:\n",
    "    total_ds (Dataset): The dataset containing the bands.\n",
    "    bands (list): A list of bands for which to compute the medians and standard deviations.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary where the keys are the bands and the values are tuples containing the median and standard deviation.\n",
    "\n",
    "    \"\"\"\n",
    "    medians_vars = {}\n",
    "    for band in bands:\n",
    "        median = total_ds[band].mean().compute()\n",
    "        std = total_ds[band].std().compute()\n",
    "        medians_vars[band] = (median, std)\n",
    "        print(f\"Band: {band}, Median: {median}, std: {std}\")\n",
    "    return medians_vars\n",
    "\n",
    "def create_filter_condition(total_ds, medians_vars):\n",
    "    \"\"\"\n",
    "    Create a filter condition based on the median and standard deviation of each band in the dataset.\n",
    "\n",
    "    Args:\n",
    "        total_ds (pandas.DataFrame): The total dataset containing the bands.\n",
    "        medians_vars (dict): A dictionary containing the median and standard deviation for each band.\n",
    "\n",
    "    Returns:\n",
    "        bool: The filter condition as a boolean value.\n",
    "\n",
    "    \"\"\"\n",
    "    filter_condition = True\n",
    "    for band, (median, std) in medians_vars.items():\n",
    "        band_condition_1 = total_ds[band] <= (median + std)\n",
    "        band_condition_2 = total_ds[band] >= (median - std)\n",
    "        filter_condition = filter_condition & band_condition_1 & band_condition_2\n",
    "    return filter_condition\n",
    "\n",
    "def apply_filter(total_ds, filter_condition):\n",
    "    \"\"\"\n",
    "    Apply a filter condition to a given dataset.\n",
    "\n",
    "    Parameters:\n",
    "    total_ds (pandas.DataFrame): The total dataset to filter.\n",
    "    filter_condition (str): The condition to filter the dataset.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: The filtered dataset.\n",
    "    \"\"\"\n",
    "    filtered_ds = total_ds[filter_condition]\n",
    "    print(f\"Filtered dataset length: {len(filtered_ds)}\")\n",
    "    return filtered_ds\n",
    "\n",
    "def subtract_swir2(filtered_ds):\n",
    "    \"\"\"\n",
    "    Subtract the 'swir1' band from each band in the given dataset.\n",
    "\n",
    "    Args:\n",
    "        filtered_ds (xarray.Dataset): The dataset containing the bands to be subtracted.\n",
    "\n",
    "    Returns:\n",
    "        xarray.Dataset: The dataset with each band subtracted by the 'swir1' band.\n",
    "    \"\"\"\n",
    "    bands = [\"blue\", \"green\", \"red\", \"swir1\", \"nir\"]\n",
    "    for band in bands:\n",
    "        filtered_ds[band] = filtered_ds[band] - filtered_ds[\"swir2\"]\n",
    "    return filtered_ds\n",
    "\n",
    "def filter_data(filtered_ds):\n",
    "    \"\"\"\n",
    "    Filter the given dataset based on specific conditions.\n",
    "\n",
    "    Parameters:\n",
    "    filtered_ds (pandas.DataFrame): The dataset to be filtered.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: The filtered dataset.\n",
    "    \"\"\"\n",
    "    filtered_ds = filtered_ds[filtered_ds[\"type\"] != \"Facility\"]\n",
    "    filtered_ds = filtered_ds[filtered_ds[\"doc\"] >= 0]\n",
    "    filtered_ds = filtered_ds[\n",
    "        (filtered_ds['type'] != 'Lake') |\n",
    "        ((filtered_ds['type'] == 'Lake') & (filtered_ds['timediff'].abs() <= 72))\n",
    "    ]\n",
    "    filtered_ds = filtered_ds[\n",
    "        (filtered_ds['type'] != 'Stream') |\n",
    "        ((filtered_ds['type'] == 'Stream') & (filtered_ds['timediff'].abs() <= 24))\n",
    "    ]\n",
    "    filtered_ds = filtered_ds[\n",
    "        (filtered_ds['type'] != 'Estuary') |\n",
    "        ((filtered_ds['type'] == 'Estuary') & (filtered_ds['timediff'].abs() <= 3))\n",
    "    ]\n",
    "    print(f\"Final filtered dataset length: {len(filtered_ds)}\")\n",
    "    return filtered_ds\n",
    "\n",
    "def process_data():\n",
    "    \"\"\"\n",
    "    Process the data by performing various operations on the total_ds DataFrame.\n",
    "\n",
    "    This function reads a CSV file called 'total_ds.csv' using Dask DataFrame,\n",
    "    computes medians and variances for specified bands, creates a filter condition,\n",
    "    applies the filter condition to the total_ds DataFrame, subtracts the 'swir1'\n",
    "    column from the filtered DataFrame, and applies additional data filtering.\n",
    "\n",
    "    The filtered DataFrame is then printed with the length and saved as a CSV file\n",
    "    called 'filtered_ds.csv'.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    total_ds = dd.read_csv('total_ds.csv', assume_missing=True)\n",
    "    bands = [\"blue\", \"green\", \"red\", \"swir1\", \"swir2\", \"nir\"]\n",
    "\n",
    "    medians_vars = compute_medians_and_vars(total_ds, bands)\n",
    "    filter_condition = create_filter_condition(total_ds, medians_vars)\n",
    "    filtered_ds = apply_filter(total_ds, filter_condition)\n",
    "    filtered_ds = subtract_swir2(filtered_ds)\n",
    "    filtered_ds = filter_data(filtered_ds)\n",
    "    filtered_ds = filtered_ds[filtered_ds[\"pixelCount\"] >= 9]\n",
    "\n",
    "    with ProgressBar():\n",
    "        print(len(filtered_ds))\n",
    "    filtered_ds.to_csv('filtered_ds.csv', index=False, single_file=True)\n",
    "\n",
    "process_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removal of unecessary columns\n",
    "\n",
    "In this step we remove all the columns that are not needed for the model. We also transform the DOC column into to a $\\log_{10}$ scale. Any missing values are filled with the mean of the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Read the filtered dataset\n",
    "total_ds_path = \"your_path\"\n",
    "# filtered_ds.csv\n",
    "filtered_ds = pd.read_csv(\"filtered_ds.csv\")\n",
    "\n",
    "# Drop unnecessary columns\n",
    "columns_to_drop = [\n",
    "    \"Unnamed: 0\", \"date_unity\", \"path\", \"qa\", \"qa_sd\", \"row\", \"sat\", \".geo\",\n",
    "    \"endtime\", \"date\", \"chl_a\", \"p_sand\", \"secchi\", \"tis\", \"tss\", \"latitude_left\",\n",
    "    \"longitude_left\", \"latitude_right\", \"longitude_right\", \"clouds\", \"time_right\",\n",
    "    \"time_left\", \"geometry\", \"pwater\", \"id\", \"index_right\", \"valid_time\",\n",
    "    \"blue_sd\", \"green_sd\", \"red_sd\", \"swir1_sd\", \"swir2_sd\", \"nir_sd\", \"pixelCount\",\n",
    "    \"distance\", \"date_utc\", \"distance\", \"date_only\", \"type\", \"timediff\"\n",
    "]\n",
    "filtered_ds = filtered_ds.drop(columns=columns_to_drop)\n",
    "\n",
    "# Calculate wind speed\n",
    "filtered_ds[\"wind\"] = np.sqrt(filtered_ds[\"u10\"]**2 + filtered_ds[\"v10\"]**2)\n",
    "filtered_ds = filtered_ds.drop(columns=[\"u10\", \"v10\"])\n",
    "\n",
    "# Process each column\n",
    "for column in filtered_ds.columns:\n",
    "    if column == \"doc\":\n",
    "        continue\n",
    "    print(f\"Processing column: {column}\")\n",
    "    # Drop rows with missing values\n",
    "    filtered_ds = filtered_ds.dropna(subset=[column])\n",
    "    \n",
    "    # Fill missing values with the column mean\n",
    "    filtered_ds[column] = filtered_ds[column].fillna(filtered_ds[column].mean())\n",
    "    \n",
    "    # Replace remaining NaN values with the mean\n",
    "    nan_mask = filtered_ds[column].isna()\n",
    "    valid_data = filtered_ds[~nan_mask]\n",
    "    mean_value = valid_data[column].mean()\n",
    "    filtered_ds[column] = filtered_ds[column].fillna(mean_value)\n",
    "    \n",
    "# Apply logarithmic transformation to the \"doc\" column\n",
    "filtered_ds[\"doc\"] = np.log10(filtered_ds[\"doc\"])\n",
    "\n",
    "# Save the processed dataset\n",
    "filtered_ds.to_csv(\"machine_learning_ds.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVR MODEL\n",
    "\n",
    "In this step we train the SVR model. We use optuna to find the best hyperparameters for the model. The output will be the best hyperparameters and the model itself. It will be created a database to track the model evolution, and can be done by running the command `optuna-dashboard sqlite:///db.sqlite3` in the dash board. The best model will pe pickled and saved in the `models` folder. It is using the sklearnx to train, which is a version of sklearn made by intel to optimize the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"your_path\"\n",
    "# machine_learning_ds.csv\n",
    "patch_sklearn()\n",
    "\n",
    "storage = \"sqlite:///db.sqlite3\"\n",
    "X_scaler = StandardScaler(data_path)\n",
    "data = pd.read_csv('')\n",
    "X = data.drop('doc', axis=1)\n",
    "y = data['doc']\n",
    "\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,\n",
    "                                                    random_state=42)\n",
    "kf_inner = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "X_train = X_scaler.fit_transform(X_train)\n",
    "X_test = X_scaler.transform(X_test)\n",
    "X_train = pd.DataFrame(X_train, columns=X.columns)\n",
    "X_test = pd.DataFrame(X_test, columns=X.columns)\n",
    "\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "def objective(trial):\n",
    "    # Parameters\n",
    "    params = {\n",
    "        'C': trial.suggest_float('C', 0.1, 10),\n",
    "        'gamma': trial.suggest_categorical('gamma', ['scale', 'auto']),\n",
    "        'tol': trial.suggest_float('tol', 1e-5, 1e-3),\n",
    "        'epsilon': trial.suggest_float('epsilon', 0.1, 1.0),\n",
    "        'shrinking': trial.suggest_categorical('shrinking', [True, False]),\n",
    "    }\n",
    "    # Model\n",
    "    model = SVR(**params, kernel='rbf', cache_size=1024)\n",
    "    # Cross Val\n",
    "    score = cross_val_score(model, X_train, y_train, cv=kf_inner.split(X_train),\n",
    "                                n_jobs=-1, scoring='r2')\n",
    "    score = score.mean()\n",
    "    return score\n",
    "\n",
    "\n",
    "# Study\n",
    "study = optuna.create_study(direction='maximize', study_name='svr_study',\n",
    "                            storage=storage, load_if_exists=True,\n",
    "                            sampler=optuna.samplers.TPESampler(seed=42))\n",
    "study.optimize(objective, n_trials = 100 ,n_jobs=-1, show_progress_bar=True)\n",
    "# Best Params\n",
    "print(f'Best Params: {study.best_params}')\n",
    "print(f'Best Score: {study.best_value}')\n",
    "\n",
    "# Model\n",
    "model = SVR(**study.best_params)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# pred\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "#rmse = root_mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "cross_val = cross_val_score(model, X, y, cv=10, n_jobs=-1, scoring='r2',\n",
    "                            verbose=2)\n",
    "print(f'RMSE: {rmse}')\n",
    "print(f'MAE: {mae}')\n",
    "print(f'R2 Score: {r2}')\n",
    "print(f'Cross Val Score: {cross_val}')\n",
    "print(f'Cross Val Mean: {cross_val.mean()}')\n",
    "print(f'Cross Val Std: {cross_val.std()}')\n",
    "\n",
    "# Save the model\n",
    "os.mkdir('models')\n",
    "joblib.dump(model, 'models/svr_model.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Model\n",
    "\n",
    "In this step we train the Random Forest model. We use optuna to find the best hyperparameters for the model. The output will be the best hyperparameters and the model itself. It will be created a database to track the model evolution, and can be done by running the command `optuna-dashboard sqlite:///db.sqlite3` in the dash board. The best model will pe pickled and saved in the `models` folder. It is using the sklearnx to train, which is a version of sklearn made by intel to optimize the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_sklearn()\n",
    "data_path = 'your_path'\n",
    "# machine_learning_ds.csv\n",
    "storage = \"sqlite:///db.sqlite3\"\n",
    "\n",
    "data = pd.read_csv(your_path)\n",
    "X = data.drop('doc', axis=1)\n",
    "y = data['doc']\n",
    "X_scaler = StandardScaler()\n",
    "\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,\n",
    "                                                    random_state=42)\n",
    "kf_inner = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "X_train = X_scaler.fit_transform(X_train)\n",
    "X_test = X_scaler.transform(X_test)\n",
    "\n",
    "\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "def objective(trial):\n",
    "    # Parameters\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 4000),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 600),\n",
    "        'min_samples_split': trial.suggest_int('min_samples_split', 2, 200),\n",
    "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 200),\n",
    "        'max_features': trial.suggest_categorical('max_features',\n",
    "                                                 ['sqrt', 'log2']),\n",
    "        'max_leaf_nodes': trial.suggest_int('max_leaf_nodes', 10, 1000),\n",
    "    }\n",
    "    \n",
    "    \n",
    "    # Model\n",
    "    model = RandomForestRegressor(**params)\n",
    "    # Score \n",
    "    score = cross_val_score(model, X_train, y_train, cv=kf_inner,\n",
    "                            scoring='r2', n_jobs=-1)\n",
    "    score = score.mean()\n",
    "    return score\n",
    "\n",
    "\n",
    "\n",
    "# Study\n",
    "study = optuna.create_study(study_name='rf_study', storage=storage,\n",
    "                            direction='maximize', load_if_exists=True,\n",
    "                            sampler=optuna.samplers.TPESampler())\n",
    "study.optimize(objective, n_trials=100, n_jobs=-1, show_progress_bar=True)\n",
    "# Best Params\n",
    "print(f'Best Params: {study.best_params}')\n",
    "print(f'Best Score: {study.best_value}')\n",
    "\n",
    "# Model\n",
    "model = RandomForestRegressor(**study.best_params)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# pred\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "#rmse = root_mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "cross_val = cross_val_score(model, X, y, cv=10, n_jobs=-1, scoring='r2',\n",
    "                            verbose=2)\n",
    "print(f'RMSE: {rmse}')\n",
    "print(f'MAE: {mae}')\n",
    "print(f'R2 Score: {r2}')\n",
    "print(f'Cross Val Score: {cross_val}')\n",
    "print(f'Cross Val Mean: {cross_val.mean()}')\n",
    "print(f'Cross Val Std: {cross_val.std()}')\n",
    "\n",
    "# Save the model\n",
    "os.mkdir('models')\n",
    "joblib.dump(model, 'rf_model.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost Model\n",
    "\n",
    "In this step we train the XGBoost model. We use optuna to find the best hyperparameters for the model. The output will be the best hyperparameters and the model itself. It will be created a database to track the model evolution, and can be done by running the command `optuna-dashboard sqlite:///db.sqlite3` in the dash board. The best model will pe pickled and saved in the `models` folder. This model is trained using the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"your_path\"\n",
    "# machine_learning_ds.csv\n",
    "storage = \"sqlite:///db.sqlite3\"\n",
    "data = pd.read_csv(data_path)\n",
    "rs = 42\n",
    "folds = 10\n",
    "n_iter = 10\n",
    "X_scaler = StandardScaler()\n",
    "\n",
    "X = data.drop('doc', axis=1)\n",
    "y = data[\"doc\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42,\n",
    "                                                    test_size=0.3)\n",
    "\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "X_train = X_scaler.fit_transform(X_train)\n",
    "X_test = X_scaler.transform(X_test)\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        'normalize_type': trial.suggest_categorical('normalize_type', ['tree', 'forest']),\n",
    "        'rate_drop': trial.suggest_float('rate_drop', 0.1, 0.9),\n",
    "        'skip_drop': trial.suggest_float('skip_drop', 0.1, 0.9),\n",
    "        'eta': trial.suggest_float('eta', 0.01, 0.9),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 20),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 400),\n",
    "        'gamma': trial.suggest_float('gamma', 0, 0.5),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1),\n",
    "    }\n",
    "\n",
    "    xgbr = XGBRegressor(tree_method='hist', eval_metric='rmse',\n",
    "                        objective='reg:squarederror', device=\"cuda\",\n",
    "                        verbosity=1, booster='dart', **params)\n",
    "\n",
    "    rmse = cross_val_score(xgbr, X_train, y_train, cv=kf.split(X_train, y_train),\n",
    "                           scoring='r2')\n",
    "\n",
    "    return rmse.mean()\n",
    "\n",
    "study = optuna.create_study(direction='maximize', study_name='XGBoost',\n",
    "                            storage=storage, load_if_exists=True,\n",
    "                            sampler=optuna.samplers.TPESampler())\n",
    "study.optimize(objective, n_trials=100, show_progress_bar=True)\n",
    "\n",
    "# Train model with best hyperparameters\n",
    "best_params = study.best_params\n",
    "model = XGBRegressor(tree_method='hist', eval_metric='rmse',\n",
    "                           objective='reg:squarederror', device=\"cuda\",\n",
    "                            verbosity=1, booster='dart', **best_params)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "\n",
    "# Best hyperparameters\n",
    "print(study.best_params)\n",
    "print(study.best_value)\n",
    "print(study.best_trial)\n",
    "\n",
    "\n",
    "# Evaluate model\n",
    "y_pred = model.predict(cupy.array(X_test))\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "cross_val = cross_val_score(model, cupy.array(X_test), y_test, cv=10, scoring='r2',\n",
    "                            verbose=2)\n",
    "print(f'RMSE: {rmse}')\n",
    "print(f'MAE: {mae}')\n",
    "print(f'R2 Score: {r2}')\n",
    "print(f'Cross Val Score: {cross_val}')\n",
    "print(f'Cross Val Mean: {cross_val.mean()}')\n",
    "print(f'Cross Val Std: {cross_val.std()}')\n",
    "\n",
    "\n",
    "model.save_model('XGBoost.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Scatter plot from models\n",
    "\n",
    "We will get the scatter plot from the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_path = 'your_path'\n",
    "#machine_learning_ds.csv\n",
    "models = ['svr_model.pkl', 'rf_model.pkl', 'XGBoost.model']\n",
    "titles = ['SVR', 'Random Forest', 'XGBoost']\n",
    "# Random state\n",
    "rs = 42\n",
    "# Load data\n",
    "df = pd.read_csv(df_path)\n",
    "X = df.drop(\"doc\", axis=1)\n",
    "y = df[\"doc\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=rs)\n",
    "\n",
    "def plot_seaborn(y_test, y_pred, title, ax=None):\n",
    "    y_pred = 10 ** y_pred\n",
    "    y_test = 10 ** y_test\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    bias = np.mean( - y_test + y_pred)\n",
    "    print(f\"RMSE of {title}: {rmse}\")\n",
    "    print(f\"MAE of {title}: {mae}\")\n",
    "    print(f\"Bias of {title}: {bias}\")\n",
    "    y_pred = np.log10(y_pred)\n",
    "    y_test = np.log10(y_test)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    print(f\"R2 Score of {title}: {r2_score(y_test, y_pred)}\")\n",
    "    g = sns.regplot(x=y_test, y=y_pred, scatter_kws={'alpha':0.7, 'color':'b'},\n",
    "                    line_kws={'color':'r', 'lw':2}, ax=ax)\n",
    "    g.set(xlabel='Real', ylabel='Predição')\n",
    "    g.set_title(title)\n",
    "    g.set_xticks(np.arange(-1, 2, 1), [fr\"$10^{{{loc}}}$\" for loc in np.arange(-1, 2, 1)])\n",
    "    g.set_yticks(np.arange(-1, 2, 1), [fr\"$10^{{{loc}}}$\" for loc in np.arange(-1, 2, 1)])\n",
    "    g.plot([y_test.min(), 2], [y_test.min(), 2], 'k--', lw=2)\n",
    "    g.set_xlim(-1, 2)\n",
    "    g.set_ylim(-1, 2)\n",
    "    g.annotate(fr\"$R^{2}: {r2:.2f}$\", xy=(0.15, 0.9), xycoords='axes fraction',\n",
    "                fontsize=16, ha='center', va='center')\n",
    "    return g\n",
    "fig, axs = plt.subplots(1, 3, figsize=(15, 5), constrained_layout=True)\n",
    "for model, title, ax in zip(models, titles, axs):\n",
    "    if model == 'XGBoost.model':\n",
    "        model = XGBRegressor()\n",
    "        model.load_model('XGBoost.json')\n",
    "    else:\n",
    "        model = joblib.load(model)\n",
    "    y_pred = model.predict(X_test)\n",
    "    plot_seaborn(y_test, y_pred, title, ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Importance\n",
    "\n",
    "We will get the feature importance from the models. We use the permutation importance to get the feature importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "csv_path = \"your_trainning_ds.csv\"\n",
    "plt.style.use(['science', 'notebook', 'grid'])\n",
    "data = pd.read_csv(csv_path)\n",
    "models = ['svr_model.pkl', 'rf_model.pkl', 'XGBoost.model']\n",
    "titles = ['SVR', 'Random Forest', 'XGBoost']\n",
    "X = data.drop('doc', axis=1)\n",
    "y = data['doc']\n",
    "X_scaler = StandardScaler()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,\n",
    "                                                    random_state=42)\n",
    "X_train = X_scaler.fit_transform(X_train)\n",
    "X_test = X_scaler.transform(X_test)\n",
    "X_train = pd.DataFrame(X_train, columns=X.columns)\n",
    "X_test = pd.DataFrame(X_test, columns=X.columns)\n",
    "model = joblib.load('rf_model.pkl')\n",
    "\n",
    "def plot_feature_importance(model, X, y, title, ax=None, scoring=None):\n",
    "    result = permutation_importance(model, X, y, n_repeats=10,\n",
    "                                    random_state=42, n_jobs=-1,\n",
    "                                    scoring=scoring)\n",
    "    results = pd.DataFrame(result.importances_mean, index=X.columns,\n",
    "                            columns=['importance'])\n",
    "    #results.sort_values(by='importance', inplace=True)\n",
    "    results.plot.bar(yerr=result.importances_std, ax=ax, legend=False)\n",
    "    ax.set_xlabel('Importance')\n",
    "    ax.set_ylabel('Features')\n",
    "    ax.set_title('{} Feature Importance'.format(title))\n",
    "    return results\n",
    "fig, axs = plt.subplots(1, 3, figsize=(15, 5), constrained_layout=True)\n",
    "for model, title, ax in zip(models, titles, axs):\n",
    "    if model == 'XGBoost.model':\n",
    "        model = XGBRegressor()\n",
    "        model.load_model('XGBoost.json')\n",
    "    else:\n",
    "        model = joblib.load(model)\n",
    "    if title == 'SVR':\n",
    "        scoring = 'neg_mean_squared_error'\n",
    "    else:\n",
    "        scoring = None\n",
    "    plot_feature_importance(model, X_test, y_test, title, ax=ax, scoring=scoring)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get some visualizations\n",
    "\n",
    "Get some visualizations from the data, to compare with the original article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "csv_path = \"your_path_to_ds.csv\"\n",
    "\n",
    "data = pd.read_csv(csv_path)\n",
    "\n",
    "bands_dict = {\n",
    "    \"blue\": \"b\", \"green\": \"g\", \"red\": \"r\", \"nir\": \"m\", \"swir1\": \"y\", \"swir2\": \"k\"\n",
    "}\n",
    "    \n",
    "\n",
    "def plot_band_histograms(data, bands_dict):\n",
    "    \"\"\"\n",
    "    Plots histograms for each band in the given DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - data (pandas.DataFrame): The DataFrame containing the bands data.\n",
    "    - bands_dict (dict): A dictionary mapping band names to colors.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    fig, axs = plt.subplots(2, 3, figsize=(15, 10), constrained_layout=True)\n",
    "    for band, ax, color in zip(bands_dict.keys(), axs.flatten(), bands_dict.values()):\n",
    "        ax.hist(data[band], bins=50, color=color, alpha=0.7)\n",
    "        ax.set_title(f\"{band.capitalize()} Band\")\n",
    "        ax.set_xlabel(\"Pixel Value\")\n",
    "        ax.set_ylabel(\"Frequency\")\n",
    "        ax.axvline(data[band].mean(), color='k', linestyle='dashed', linewidth=1)\n",
    "        ax.axvline(data[band].mean() + data[band].std(), color='k',\n",
    "                   linestyle='dashed', linewidth=1)\n",
    "        ax.axvline(data[band].mean() - data[band].std(), color='k',\n",
    "                     linestyle='dashed', linewidth=1)\n",
    "        ax.annotate(fr\"$\\mu: {data[band].mean():.2f}$\" + \"\\n\" +\n",
    "                    fr\"$\\sigma: {data[band].std():.2f}$\",\n",
    "                    xy=(0.75, 0.9), xycoords='axes fraction',\n",
    "                    ha='center', va='center', fontsize=16)\n",
    "        \n",
    "    plt.show()\n",
    "def plot_doc_distribution(data, color = 'b'):\n",
    "    \"\"\"\n",
    "    Plots the distribution of the 'doc' column in the given DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - data (pandas.DataFrame): The DataFrame containing the 'doc' column.\n",
    "    - color (str): The color to use for the plot.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.hist(data['doc'], bins=50, color=color, alpha=0.7,\n",
    "             edgecolor='k', linewidth=1.5)\n",
    "    plt.title(\"Distribution of DOC Values\")\n",
    "    plt.xlabel(\"DOC Value\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.annotate(fr\"$\\mu: {data['doc'].mean():.2f}$\" + \"\\n\" +\n",
    "                 fr\"$\\sigma: {data['doc'].std():.2f}$\" + \"\\n\" +\n",
    "                 fr\"$\\eta: {data['doc'].median():.2f}$\",\n",
    "                 xy=(0.75, 0.9), xycoords='axes fraction',\n",
    "                 ha='center', va='center', fontsize=16)\n",
    "    plt.xticks(np.arange(-1, 3, 1), [fr\"$10^{{{loc}}}$\" for loc in np.arange(-1, 3, 1)])\n",
    "    plt.show()\n",
    "\n",
    "plot_band_histograms(data, bands_dict)\n",
    "plot_doc_distribution(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Summary statistics\n",
    "\n",
    "Plot all the summary statistics from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sns.color_palette(\"hls\", 8)\n",
    "data_path = \"your_path_to_ds.csv\"\n",
    "data = pd.read_csv(data_path)\n",
    "def plt_distribution(x, ax, title, var_name, kde = False, color = 'b'):\n",
    "    sns.histplot(x, kde=kde, ax=ax, color=color)\n",
    "    mean = x.mean()\n",
    "    std = x.std()\n",
    "    median = x.median()\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(var_name)\n",
    "    ax.set_ylabel('Frequency')\n",
    "    ax.annotate(fr\"$\\mu: {mean:.2E}$\", xy=(0.8, 0.95), xycoords='axes fraction',\n",
    "                 fontsize=16, ha = \"center\", va=\"center\")\n",
    "    ax.annotate(fr\"$\\sigma: {std:.2E}$\", xy=(0.8, 0.9), xycoords='axes fraction',\n",
    "                    fontsize=16, ha = \"center\", va=\"center\")\n",
    "    ax.annotate(fr\"$\\eta: {median:.2E}$\", xy=(0.8, 0.85), xycoords='axes fraction',\n",
    "                fontsize=16, ha = \"center\", va=\"center\")\n",
    "    return ax\n",
    "\n",
    "X = data.drop('doc', axis=1)\n",
    "num_var = X.columns\n",
    "fig, axs = plt.subplots(4, 4, figsize=(30, 30), constrained_layout=True,\n",
    "                        dpi=300)\n",
    "for var, ax in zip(num_var, axs.ravel()):\n",
    "    plt_distribution(data[var], ax, var, var_name=var)\n",
    "    mean = data[var].mean()\n",
    "    std = data[var].std()\n",
    "    median = data[var].median()\n",
    "if len(num_var) != len(axs.ravel()):\n",
    "    for ax in axs.ravel()[len(num_var):]:\n",
    "        ax.set_visible(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_path = \"your_path_to_ds.csv\"\n",
    "data = pd.read_csv(data_path)\n",
    "# this data need to have this collumns. If you didnt\n",
    "# change the names, you can import the \"filtered_ds.csv\"\n",
    "    \n",
    "data[\"date_utc\"] = pd.to_datetime(data[\"date_utc\"])\n",
    "date = data[\"date_utc\"]\n",
    "year = date.dt.year\n",
    "month = date.dt.month\n",
    "hour = date.dt.hour\n",
    "\n",
    "landsat = data[\"sat\"]\n",
    "location = data[\"type\"]\n",
    "fig, ax = plt.subplots(5, 1, figsize=(16, 16), constrained_layout=True)\n",
    "ax[0].hist(year, bins=35, color='b', alpha=0.7,\n",
    "            histtype='bar', ec='black')\n",
    "ax[0].set_title(\"Year\")\n",
    "ax[0].set_xlabel(\"Year\")\n",
    "ax[0].set_ylabel(\"Frequency\")\n",
    "ax[1].hist(month, bins=12, color='r', alpha=0.7,\n",
    "            histtype='bar', ec='black')\n",
    "ax[1].set_title(\"Month\")\n",
    "ax[1].set_xlabel(\"Month\")\n",
    "ax[1].set_ylabel(\"Frequency\")\n",
    "\n",
    "ax[2].hist(hour, bins=24, color='g', alpha=0.7,\n",
    "            histtype='bar', ec='black')\n",
    "ax[2].set_title(\"Hour\")\n",
    "ax[2].set_xlabel(\"Hour\")\n",
    "ax[2].set_ylabel(\"Frequency\")\n",
    "\n",
    "ax[3].bar(landsat.value_counts().index, landsat.value_counts().values, color='purple', alpha=0.7)\n",
    "ax[3].set_title(\"Landsat\")\n",
    "ax[3].set_xlabel(\"Landsat Satellite\")\n",
    "ax[3].set_ylabel(\"Frequency\")\n",
    "\n",
    "ax[4].bar(location.value_counts().index, location.value_counts().values, color='orange', alpha=0.7)\n",
    "ax[4].set_title(\"Location of Sample\")\n",
    "ax[4].set_xlabel(\"Location\")\n",
    "ax[4].set_ylabel(\"Frequency\")\n",
    "\n",
    "ax[0].set_xticks(np.arange(1984, 2020, 5))\n",
    "ax[0].set_xticklabels([str(i) for i in np.arange(1984, 2020, 5)])\n",
    "\n",
    "ax[1].set_xticks(np.arange(1, 13, 1))\n",
    "months_name = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep',\n",
    "                'Oct', 'Nov', 'Dec']\n",
    "ax[1].set_xticklabels(months_name)\n",
    "\n",
    "ax[2].set_xticks(np.arange(0, 24, 1))\n",
    "\n",
    "\n",
    "ax[3].set_xticks([5, 7, 8])\n",
    "landsat_name = ['Landsat 5', 'Landsat 7', 'Landsat 8']\n",
    "\n",
    "ax[3].set_xticklabels(landsat_name)\n",
    "ax[3].set_xlabel(\"Landsat Satellite\")\n",
    "\n",
    "ax[4].set_xticks([0, 1, 2])\n",
    "location_name = ['Stream', 'Lake', 'Estuary']\n",
    "ax[4].set_xticklabels(location_name)\n",
    "ax[4].set_xlabel(\"Location of Sample\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot trainning curve from models\n",
    "\n",
    "Plot the trainning curve from the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "so.Plot.config.theme.update(sns.axes_style('whitegrid') | sns.plotting_context('notebook'))\n",
    "plt.style.use(['science', 'notebook', 'grid'])\n",
    "titles = ['SVR', 'Random Forest', 'XGBoost']\n",
    "storage = \"sqlite:///db.sqlite3\"\n",
    "studies = ['svr_study', 'rf_study', 'XGBoost']\n",
    "\n",
    "f = mpl.figure.Figure(figsize=(16, 9))\n",
    "sf1, sf2, sf3 = f.subfigures(1, 3)\n",
    "\n",
    "svr_study = optuna.load_study(study_name=\"svr_study\", storage=storage)\n",
    "df = svr_study.trials_dataframe()\n",
    "(so.Plot(data=df, x='number', y='value')\n",
    "    .add(so.Dot(marker='o'))\n",
    "    .add(so.Line(color='blue'), so.PolyFit(order=2))\n",
    "    .label(title=\"SVR optimization\", x_label='Number of Trial',\n",
    "           y_label='Objective Value')\n",
    "    .on(sf1)\n",
    "    .plot())\n",
    "\n",
    "rf_study = optuna.load_study(study_name=\"rf_study\", storage=storage)\n",
    "df = rf_study.trials_dataframe()\n",
    "(so.Plot(data=df, x='number', y='value')\n",
    "    .add(so.Dot(marker='o'))\n",
    "    .add(so.Line(color='blue'), so.PolyFit(order=3))\n",
    "    .label(title=\"Random Forest optimization\", x_label='Number of Trial',\n",
    "           y_label='Objective Value')\n",
    "    .on(sf2)\n",
    "    .plot())\n",
    "\n",
    "XGB_study = optuna.load_study(study_name=\"XGBoost\", storage=storage)\n",
    "df = XGB_study.trials_dataframe()\n",
    "(so.Plot(data=df, x='number', y='value')\n",
    "    .add(so.Dot(marker='o'))\n",
    "    .add(so.Line(color='blue'), so.PolyFit(order=3))\n",
    "    .label(title=\"XGBoost optimization\", x_label='Number of Trial',\n",
    "           y_label='Objective Value')\n",
    "    .on(sf3)\n",
    "    .plot())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Ic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
